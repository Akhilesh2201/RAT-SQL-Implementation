{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gjz7Nhxcj6O6"
      },
      "outputs": [],
      "source": [
        "# Install and Import Dependencies\n",
        "\n",
        "!pip install -q transformers datasets gradio torchtext sqlalchemy nltk accelerate\n",
        "\n",
        "import os\n",
        "import json\n",
        "import sqlite3\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from typing import List, Dict, Tuple\n",
        "import gradio as gr\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Spider Dataset\n",
        "\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/spider_data.zip\"\n",
        "extract_dir = \"/content/spider\"\n",
        "\n",
        "if not os.path.exists(extract_dir):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        for member in zip_ref.namelist():\n",
        "            if member.startswith(\"spider_data/\") and not member.endswith(\"/\"):\n",
        "                rel_path = os.path.relpath(member, \"spider_data\")\n",
        "                out_path = os.path.join(extract_dir, rel_path)\n",
        "                os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "                with open(out_path, \"wb\") as f:\n",
        "                    f.write(zip_ref.read(member))\n",
        "    print(\"Extracted spider_data â†’\", extract_dir)\n",
        "else:\n",
        "    print(\"Already extracted.\")\n",
        "\n",
        "# Load helper\n",
        "def load_json(filename):\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "spider_dir = \"/content/spider\"\n",
        "db_dir = os.path.join(spider_dir, \"database\")\n",
        "\n",
        "train_data = load_json(os.path.join(spider_dir, \"train_spider.json\"))\n",
        "dev_data = load_json(os.path.join(spider_dir, \"dev.json\"))\n",
        "table_schemas = load_json(os.path.join(spider_dir, \"tables.json\"))\n",
        "schema_dict = {schema['db_id']: schema for schema in table_schemas}\n",
        "\n",
        "print(f\"Loaded {len(train_data)} training examples\")\n",
        "print(f\"Loaded {len(dev_data)} dev examples\")\n",
        "print(f\"Loaded {len(schema_dict)} schema definitions\")"
      ],
      "metadata": {
        "id": "qpu2LbMSj_8j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Schema Graph + Link Question to Schema\n",
        "\n",
        "import nltk\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    return nltk.word_tokenize(text.lower())\n",
        "\n",
        "def extract_schema_elements(schema: Dict) -> Tuple[List[str], List[str]]:\n",
        "    table_names = [t.lower() for t in schema['table_names_original']]\n",
        "    column_names = []\n",
        "    for table_id, col_name in schema['column_names_original']:\n",
        "        if col_name == \"*\":\n",
        "            continue\n",
        "        full_col = f\"{table_names[table_id]}.{col_name.lower()}\" if table_id >= 0 else col_name.lower()\n",
        "        column_names.append(full_col)\n",
        "    return column_names, table_names\n",
        "\n",
        "def get_values_from_db(db_id: str, schema: Dict, db_dir: str) -> Dict[str, List[str]]:\n",
        "    value_dict = defaultdict(list)\n",
        "    try:\n",
        "        conn = sqlite3.connect(os.path.join(db_dir, db_id, f\"{db_id}.sqlite\"))\n",
        "        cursor = conn.cursor()\n",
        "        table_names = [t.lower() for t in schema['table_names_original']]\n",
        "        for table_id, col_name in schema['column_names_original']:\n",
        "            if col_name == \"*\" or table_id < 0:\n",
        "                continue\n",
        "            table = table_names[table_id]\n",
        "            col = col_name.lower()\n",
        "            try:\n",
        "                cursor.execute(f\"SELECT DISTINCT {col} FROM {table} LIMIT \")\n",
        "                values = [str(row[0]).lower() for row in cursor.fetchall() if row[0] is not None]\n",
        "                value_dict[f\"{table}.{col}\"] = values\n",
        "            except Exception:\n",
        "                continue\n",
        "        conn.close()\n",
        "    except Exception as e:\n",
        "        print(f\"[DB Error] Failed to connect for {db_id}: {e}\")\n",
        "    return value_dict\n",
        "\n",
        "def get_relations(question: str, schema: Dict, db_dir: str) -> Dict:\n",
        "    q_tokens = tokenize(question)\n",
        "    col_names, tab_names = extract_schema_elements(schema)\n",
        "    schema_tokens = col_names + tab_names\n",
        "    all_nodes = q_tokens + schema_tokens\n",
        "    edge_types = {}\n",
        "\n",
        "    try:\n",
        "        value_dict = get_values_from_db(schema['db_id'], schema, db_dir)\n",
        "        for i, q_tok in enumerate(q_tokens):\n",
        "            for full_col, values in value_dict.items():\n",
        "                if q_tok in values and full_col in schema_tokens:\n",
        "                    j = schema_tokens.index(full_col)\n",
        "                    edge_types[(i, len(q_tokens) + j)] = \"value_match\"\n",
        "                    edge_types[(len(q_tokens) + j, i)] = \"value_match\"\n",
        "    except Exception as e:\n",
        "        print(f\"[Linking] Value linking failed: {e}\")\n",
        "\n",
        "    for i, q_tok in enumerate(q_tokens):\n",
        "        for j, s_tok in enumerate(schema_tokens):\n",
        "            if q_tok in s_tok.split(\"_\") or s_tok in q_tok:\n",
        "                edge_types[(i, len(q_tokens) + j)] = \"match\"\n",
        "                edge_types[(len(q_tokens) + j, i)] = \"match\"\n",
        "\n",
        "    for i, col in enumerate(col_names):\n",
        "        for j, tab in enumerate(tab_names):\n",
        "            if col.startswith(tab + \".\"):\n",
        "                ci = len(q_tokens) + i\n",
        "                tj = len(q_tokens) + len(col_names) + j\n",
        "                edge_types[(ci, tj)] = \"belongs_to\"\n",
        "                edge_types[(tj, ci)] = \"belongs_to\"\n",
        "\n",
        "    for i, col_i in enumerate(col_names):\n",
        "        for j, col_j in enumerate(col_names):\n",
        "            if i != j and col_i.split('.')[0] == col_j.split('.')[0]:\n",
        "                ci, cj = len(q_tokens) + i, len(q_tokens) + j\n",
        "                edge_types[(ci, cj)] = \"same_table\"\n",
        "\n",
        "    for fk_pair in schema.get('foreign_keys', []):\n",
        "        col1_idx, col2_idx = fk_pair\n",
        "        if col1_idx < len(col_names) and col2_idx < len(col_names):\n",
        "            c1 = len(q_tokens) + col1_idx\n",
        "            c2 = len(q_tokens) + col2_idx\n",
        "            edge_types[(c1, c2)] = \"foreign_key_forward\"\n",
        "            edge_types[(c2, c1)] = \"foreign_key_backward\"\n",
        "\n",
        "    for pk_idx in schema.get('primary_keys', []):\n",
        "        if pk_idx < len(col_names):\n",
        "            col = len(q_tokens) + pk_idx\n",
        "            table_name = col_names[pk_idx].split('.')[0]\n",
        "            if table_name in tab_names:\n",
        "                tab = len(q_tokens) + len(col_names) + tab_names.index(table_name)\n",
        "                edge_types[(col, tab)] = \"primary_key\"\n",
        "                edge_types[(tab, col)] = \"primary_key\"\n",
        "\n",
        "    return {\n",
        "        \"tokens\": all_nodes,\n",
        "        \"edges\": edge_types,\n",
        "        \"q_len\": len(q_tokens),\n",
        "        \"schema_len\": len(schema_tokens),\n",
        "        \"column_names\": col_names,\n",
        "        \"table_names\": tab_names\n",
        "    }"
      ],
      "metadata": {
        "id": "vvl0KTZq0oCT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder(Transformer + Relation-Aware Attention)\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class EmbeddingEncoder(nn.Module):\n",
        "    def __init__(self, model_name='microsoft/deberta-v3-base'):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        self.transformer = AutoModel.from_pretrained(model_name)\n",
        "        self.output_dim = self.transformer.config.hidden_size\n",
        "\n",
        "    def forward(self, word_lists: List[List[str]]) -> Tuple[torch.Tensor, None]:\n",
        "        tokenized = self.tokenizer(\n",
        "            word_lists,\n",
        "            padding=True,\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        input_ids = tokenized['input_ids'].to(device)\n",
        "        attention_mask = tokenized['attention_mask'].to(device)\n",
        "\n",
        "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = outputs.last_hidden_state\n",
        "\n",
        "        word_embeddings = []\n",
        "        for i in range(len(word_lists)):\n",
        "            word_ids = tokenized.word_ids(batch_index=i)\n",
        "            grouped = defaultdict(list)\n",
        "            for j, wid in enumerate(word_ids):\n",
        "                if wid is not None:\n",
        "                    grouped[wid].append(last_hidden[i, j])\n",
        "            avg_embeds = [torch.stack(group).mean(0) for _, group in sorted(grouped.items())]\n",
        "            word_embeddings.append(torch.stack(avg_embeds))\n",
        "\n",
        "        max_len = max(x.size(0) for x in word_embeddings)\n",
        "        padded = torch.stack([\n",
        "            F.pad(x, (0, 0, 0, max_len - x.size(0))) for x in word_embeddings\n",
        "        ])\n",
        "        return padded.to(device), None\n",
        "\n",
        "\n",
        "class RATEncoderLayer(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, dropout=0.1, num_relations=10):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "        self.rel_k = nn.Embedding(num_relations, dim)\n",
        "        self.rel_v = nn.Embedding(num_relations, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(dim, 4 * dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * dim, dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, rel_mat: torch.Tensor) -> torch.Tensor:\n",
        "        B, L, D = x.size()\n",
        "        rel_kv = self.rel_k(rel_mat)  # [B, L, L, D]\n",
        "        rel_vv = self.rel_v(rel_mat)\n",
        "\n",
        "        q = x  # [B, L, D]\n",
        "        k = x.unsqueeze(2) + rel_kv  # [B, L, L, D]\n",
        "        v = x.unsqueeze(2) + rel_vv\n",
        "\n",
        "        k = k.view(B * L, L, D)\n",
        "        v = v.view(B * L, L, D)\n",
        "        q = q.view(B * L, 1, D)\n",
        "\n",
        "        out, _ = self.attn(q, k, v)\n",
        "        out = out.view(B, L, D)\n",
        "\n",
        "        x = self.norm1(x + self.dropout(out))\n",
        "        x = self.norm2(x + self.dropout(self.ffn(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class RATEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_layers=4, num_relations=10):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layers = nn.ModuleList([\n",
        "            RATEncoderLayer(hidden_dim, num_heads=8, dropout=0.1, num_relations=num_relations)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.Tensor, rel_mat: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.input_proj(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, rel_mat)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "PS-2cWPx0ovE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder(Not AST)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sqlparse\n",
        "import re\n",
        "\n",
        "RULES = {\n",
        "    'ROOT': ['sql'],\n",
        "    'sql': ['select', 'from', 'where', 'group_by', 'having', 'order_by', 'limit', 'set_op'],\n",
        "    'select': ['Select'],\n",
        "    'from': ['From'],\n",
        "    'where': ['None', 'Where'],\n",
        "    'group_by': ['None', 'GroupBy'],\n",
        "    'having': ['None', 'Having'],\n",
        "    'order_by': ['None', 'OrderBy'],\n",
        "    'limit': ['None', 'Limit'],\n",
        "    'set_op': ['None', 'Union', 'Intersect', 'Except'],\n",
        "    'cond': ['Eq', 'Gt', 'Lt', 'Ge', 'Le', 'Ne', 'And', 'Or', 'In', 'Like', 'Between', 'Exists', 'Not'],\n",
        "    'val_unit': ['Column', 'Minus', 'Plus', 'Times', 'Divide'],\n",
        "    'table_unit': ['Table', 'TableUnitSql'],\n",
        "    'agg': ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
        "}\n",
        "\n",
        "class SQLTreeDecoder(nn.Module):\n",
        "    def __init__(self, encoder_dim, hidden_dim, num_values=100):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTMCell(encoder_dim, hidden_dim)\n",
        "        self.rule_classifiers = nn.ModuleDict({\n",
        "            rule: nn.Linear(hidden_dim, len(constructors)) for rule, constructors in RULES.items()\n",
        "        })\n",
        "        self.column_pointer = nn.Linear(hidden_dim, encoder_dim)\n",
        "        self.table_pointer = nn.Linear(hidden_dim, encoder_dim)\n",
        "        self.value_generator = nn.Linear(hidden_dim, num_values)\n",
        "        self.order_direction_classifier = nn.Linear(hidden_dim, 2)\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, encoder_output, graph, beam_size=5):\n",
        "        h = encoder_output.mean(dim=1).squeeze(0)\n",
        "        c = torch.zeros_like(h)\n",
        "        h, c = self.lstm(h, (h, c))\n",
        "        beam = self.decode_node_beam(\"ROOT\", h, encoder_output, graph, beam_size=beam_size)\n",
        "        return beam[0][\"sql\"]\n",
        "\n",
        "    def forward_supervised(self, encoder_output, graph, labels):\n",
        "        loss = 0.0\n",
        "        h = encoder_output.mean(dim=1).squeeze(0)\n",
        "        c = torch.zeros_like(h)\n",
        "        h, c = self.lstm(h, (h, c))\n",
        "\n",
        "        rule_logits = self.rule_classifiers['select'](h)\n",
        "        loss += F.cross_entropy(rule_logits.unsqueeze(0), torch.tensor([0], device=h.device))\n",
        "\n",
        "        for col_idx, agg_id in zip(labels['select_cols'], labels['select_aggs']):\n",
        "            agg_logits = self.rule_classifiers['agg'](h)\n",
        "            loss += F.cross_entropy(agg_logits.unsqueeze(0), torch.tensor([agg_id], device=h.device))\n",
        "\n",
        "            col_query = self.column_pointer(h)\n",
        "            col_scores = torch.matmul(col_query, encoder_output.squeeze(0).T)\n",
        "            col_idx = min(col_idx, encoder_output.size(1) - 1)\n",
        "            loss += F.cross_entropy(col_scores.unsqueeze(0), torch.tensor([col_idx], device=h.device))\n",
        "\n",
        "            h, c = self.lstm(h, (h, c))\n",
        "\n",
        "        rule_logits = self.rule_classifiers['from'](h)\n",
        "        loss += F.cross_entropy(rule_logits.unsqueeze(0), torch.tensor([0], device=h.device))\n",
        "\n",
        "        tab_query = self.table_pointer(h)\n",
        "        tab_scores = torch.matmul(tab_query, encoder_output.squeeze(0).T)\n",
        "        tab_idx = min(labels['tab_idx'], encoder_output.size(1) - 1)\n",
        "        loss += F.cross_entropy(tab_scores.unsqueeze(0), torch.tensor([tab_idx], device=h.device))\n",
        "\n",
        "        h, c = self.lstm(h, (h, c))\n",
        "\n",
        "        if labels['conds']:\n",
        "            rule_logits = self.rule_classifiers['where'](h)\n",
        "            loss += F.cross_entropy(rule_logits.unsqueeze(0), torch.tensor([1], device=h.device))\n",
        "\n",
        "            for cond in labels['conds']:\n",
        "                op_logits = self.rule_classifiers['cond'](h)\n",
        "                op_id = RULES['cond'].index(cond['op'].capitalize()) if cond['op'].capitalize() in RULES['cond'] else 0\n",
        "                loss += F.cross_entropy(op_logits.unsqueeze(0), torch.tensor([op_id], device=h.device))\n",
        "\n",
        "                col_query = self.column_pointer(h)\n",
        "                col_scores = torch.matmul(col_query, encoder_output.squeeze(0).T)\n",
        "                col_idx = min(cond['col'], encoder_output.size(1) - 1)\n",
        "                loss += F.cross_entropy(col_scores.unsqueeze(0), torch.tensor([col_idx], device=h.device))\n",
        "\n",
        "                val_logits = self.value_generator(h)\n",
        "                loss += F.cross_entropy(val_logits.unsqueeze(0), torch.tensor([cond['val_id']], device=h.device))\n",
        "\n",
        "                h, c = self.lstm(h, (h, c))\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def decode_node_beam(self, rule, state, enc, graph, beam_size=5):\n",
        "        if rule not in self.rule_classifiers:\n",
        "            return [{\"sql\": f\"--{rule}--\", \"score\": 0.0}]\n",
        "\n",
        "        logits = self.rule_classifiers[rule](state)\n",
        "        probs = F.log_softmax(logits, dim=-1)\n",
        "        k = min(beam_size, len(RULES[rule]), probs.size(-1))\n",
        "        topk_probs, topk_indices = probs.view(-1).topk(k)\n",
        "\n",
        "        candidates = []\n",
        "        for i in range(k):\n",
        "            pred_idx = topk_indices[i].item()\n",
        "            score = topk_probs[i].item()\n",
        "            if pred_idx >= len(RULES[rule]):\n",
        "                continue\n",
        "            constructor = RULES[rule][pred_idx]\n",
        "\n",
        "            if constructor == 'Select':\n",
        "                result = self.decode_select(state, enc, graph)\n",
        "            elif constructor == 'From':\n",
        "                result = self.decode_from(state, enc, graph)\n",
        "            elif constructor == 'Where':\n",
        "                result = self.decode_where(state, enc, graph)\n",
        "            elif constructor == 'GroupBy':\n",
        "                result = self.decode_group_by(state, enc, graph)\n",
        "            elif constructor == 'Having':\n",
        "                result = self.decode_having(state, enc, graph)\n",
        "            elif constructor == 'OrderBy':\n",
        "                result = self.decode_order_by(state, enc, graph)\n",
        "            elif constructor == 'Limit':\n",
        "                result = self.decode_limit(state)\n",
        "            elif constructor in {'Union', 'Intersect', 'Except'}:\n",
        "                result = self.decode_set_op(constructor, state)\n",
        "            elif rule == 'ROOT':\n",
        "                result = self.decode_node_beam('sql', state, enc, graph)[0][\"sql\"]\n",
        "            elif rule == 'sql':\n",
        "                clauses = []\n",
        "                total_score = score\n",
        "                for clause_rule in RULES['sql']:\n",
        "                    if clause_rule not in self.rule_classifiers:\n",
        "                        continue\n",
        "                    clause_logits = self.rule_classifiers[clause_rule](state)\n",
        "                    clause_probs = F.log_softmax(clause_logits, dim=-1)\n",
        "                    clause_probs = clause_probs.view(-1)\n",
        "                    clause_idx = torch.argmax(clause_probs).item()\n",
        "                    clause_idx = min(clause_idx, len(RULES[clause_rule]) - 1)\n",
        "                    clause_score = clause_probs[clause_idx].item()\n",
        "                    clause_constructor = RULES[clause_rule][clause_idx]\n",
        "\n",
        "                    total_score += clause_score\n",
        "                    if clause_constructor != 'None':\n",
        "                        sub_beam = self.decode_node_beam(clause_rule, state, enc, graph, beam_size=1)\n",
        "                        if sub_beam and \"sql\" in sub_beam[0]:\n",
        "                            clauses.append(sub_beam[0][\"sql\"])\n",
        "                        else:\n",
        "                            print(f\"[Warning] Empty beam for clause '{clause_rule}' â€” skipping.\")\n",
        "\n",
        "                has_group_by = any(\"GROUP BY\" in c for c in clauses)\n",
        "                clauses = [c for c in clauses if not c.startswith(\"HAVING\") or has_group_by]\n",
        "\n",
        "                result = ' '.join(clauses)\n",
        "                candidates.append({\"sql\": result, \"score\": total_score})\n",
        "                return candidates\n",
        "            else:\n",
        "                result = f\"--Unhandled {constructor}--\"\n",
        "\n",
        "\n",
        "            candidates.append({\"sql\": result, \"score\": score})\n",
        "\n",
        "        return sorted(candidates, key=lambda x: -x[\"score\"])[:beam_size]\n",
        "\n",
        "    def decode_select(self, state, enc, graph, threshold=0.2):\n",
        "        agg_logits = self.rule_classifiers['agg'](state)\n",
        "        agg_probs = F.softmax(agg_logits, dim=-1)\n",
        "        col_query = self.column_pointer(state).unsqueeze(0)\n",
        "        enc_ = enc[0]\n",
        "        col_scores = torch.matmul(col_query, enc_.T).squeeze(0)\n",
        "        col_probs = F.softmax(col_scores, dim=-1).view(-1)\n",
        "        selected = [i for i, p in enumerate(col_probs.detach().cpu().tolist()) if p > threshold]\n",
        "        if not selected:\n",
        "            selected = [torch.argmax(col_scores).item()]\n",
        "        parts = []\n",
        "        for i in selected:\n",
        "            i = min(i, len(graph['column_names']) - 1)\n",
        "            col = graph['column_names'][i]\n",
        "            agg_idx = torch.argmax(agg_probs).item()\n",
        "            agg_idx = min(agg_idx, len(RULES['agg']) - 1)\n",
        "            agg = RULES['agg'][agg_idx]\n",
        "\n",
        "            parts.append(f\"{agg}({col})\" if agg else col)\n",
        "        return \"SELECT \" + \", \".join(parts)\n",
        "\n",
        "    def decode_from(self, state, enc, graph):\n",
        "        tab_query = self.table_pointer(state).unsqueeze(0)\n",
        "        enc_ = enc[0]\n",
        "        tab_scores = torch.matmul(tab_query, enc_.T).squeeze(0)\n",
        "        tab_idx = torch.argmax(tab_scores).item()\n",
        "        tab_idx = min(tab_idx, len(graph['table_names']) - 1)\n",
        "        return f\"FROM {graph['table_names'][tab_idx]}\"\n",
        "\n",
        "    def decode_where(self, state, enc, graph):\n",
        "        col_query = self.column_pointer(state).unsqueeze(0)\n",
        "        enc_ = enc[0]\n",
        "        col_scores = torch.matmul(col_query, enc_.T).squeeze(0)\n",
        "        col_idx = torch.argmax(col_scores).item()\n",
        "        col_idx = min(col_idx, len(graph['column_names']) - 1)\n",
        "        col = graph['column_names'][col_idx]\n",
        "        op_logits = self.rule_classifiers['cond'](state)\n",
        "        op_idx = torch.argmax(op_logits).item()\n",
        "        op_idx = min(op_idx, len(RULES['cond']) - 1)\n",
        "        op = RULES['cond'][op_idx]\n",
        "\n",
        "        val_logits = self.value_generator(state)\n",
        "        val_id = torch.argmax(val_logits).item()\n",
        "        val = graph.get(\"value_vocab_inv\", {}).get(val_id, f\"val{val_id}\")\n",
        "        return f\"WHERE {col} {op} '{val}'\"\n",
        "\n",
        "\n",
        "    def decode_group_by(self, state, enc, graph, threshold=0.2):\n",
        "        col_query = self.column_pointer(state).unsqueeze(0)\n",
        "        enc_ = enc[0]\n",
        "        col_scores = torch.matmul(col_query, enc_.T).squeeze(0)\n",
        "        col_probs = F.softmax(col_scores, dim=-1).view(-1)\n",
        "        selected = [i for i, p in enumerate(col_probs.detach().cpu().tolist()) if p > threshold]\n",
        "        if not selected:\n",
        "            selected = [torch.argmax(col_scores).item()]\n",
        "        cols = [graph['column_names'][i] if i < len(graph['column_names']) else f\"col{i}\" for i in selected]\n",
        "        return \"GROUP BY \" + \", \".join(cols)\n",
        "\n",
        "    def decode_having(self, state, enc, graph):\n",
        "        col_query = self.column_pointer(state).unsqueeze(0)\n",
        "        enc_ = enc[0]\n",
        "        col_scores = torch.matmul(col_query, enc_.T).squeeze(0)\n",
        "        col_idx = torch.argmax(col_scores).item()\n",
        "        col_idx = min(col_idx, len(graph['column_names']) - 1)\n",
        "        col = graph['column_names'][col_idx]\n",
        "        val_logits = self.value_generator(state)\n",
        "        val_id = torch.argmax(val_logits).item()\n",
        "        val = graph.get(\"value_vocab_inv\", {}).get(val_id, f\"val{val_id}\")\n",
        "        return f\"HAVING COUNT({col}) > '{val}'\"\n",
        "\n",
        "    def decode_order_by(self, state, enc, graph, threshold=0.2):\n",
        "        col_query = self.column_pointer(state).unsqueeze(0)\n",
        "        enc_ = enc[0]\n",
        "        col_scores = torch.matmul(col_query, enc_.T).squeeze(0)\n",
        "        col_probs = F.softmax(col_scores, dim=-1).view(-1)\n",
        "        selected = [i for i, p in enumerate(col_probs.detach().cpu().tolist()) if p > threshold]\n",
        "        if not selected:\n",
        "            selected = [torch.argmax(col_scores).item()]\n",
        "\n",
        "        dir_logits = self.order_direction_classifier(state)\n",
        "        dir_probs = F.softmax(dir_logits, dim=-1)\n",
        "\n",
        "        dir_idx = torch.argmax(dir_probs).item()\n",
        "        dir_idx = min(dir_idx, len(['ASC', 'DESC']) - 1)\n",
        "        direction = ['ASC', 'DESC'][dir_idx]\n",
        "\n",
        "        return \"ORDER BY \" + \", \".join([\n",
        "            f\"{graph['column_names'][i]} {direction}\" if i < len(graph['column_names']) else f\"col{i} {direction}\"\n",
        "            for i in selected\n",
        "        ])\n",
        "\n",
        "    def decode_limit(self, state):\n",
        "        return \"LIMIT 10\"\n",
        "\n",
        "    def decode_set_op(self, constructor, state):\n",
        "        return f\"{constructor.upper()} SELECT ...\""
      ],
      "metadata": {
        "id": "19ISvffv1Ex-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize models\n",
        "encoder_model = EmbeddingEncoder().to(device)\n",
        "rat_encoder = RATEncoder(input_dim=encoder_model.output_dim).to(device)\n",
        "decoder = SQLTreeDecoder(\n",
        "    encoder_dim=256,\n",
        "    hidden_dim=256,\n",
        "    num_values=100\n",
        ").to(device)\n",
        "decoder.num_values = 100\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = SpiderMiniDataset(train_data, schema_dict, db_dir)\n",
        "dev_dataset = SpiderMiniDataset(dev_data, schema_dict, db_dir)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "if 'value_vocab' not in globals():\n",
        "    def build_value_vocab(schema_dict, db_dir, max_size=100):\n",
        "        vocab = {}\n",
        "        idx = 1\n",
        "        for db_id, schema in schema_dict.items():\n",
        "            values = get_values_from_db(db_id, schema, db_dir)\n",
        "            for val_list in values.values():\n",
        "                for val in val_list:\n",
        "                    if val not in vocab:\n",
        "                        vocab[val] = idx\n",
        "                        idx += 1\n",
        "                    if len(vocab) >= max_size:\n",
        "                        return vocab\n",
        "        return vocab\n",
        "\n",
        "    value_vocab = build_value_vocab(schema_dict, db_dir, max_size=100)\n",
        "    value_vocab_inv = {v: k for k, v in value_vocab.items()}\n",
        "\n",
        "optimizer = torch.optim.AdamW(list(encoder_model.parameters()) +\n",
        "                              list(rat_encoder.parameters()) +\n",
        "                              list(decoder.parameters()), lr=2e-5)\n",
        "scaler = GradScaler()\n",
        "\n",
        "best_acc = 0.0\n",
        "patience = 3\n",
        "epochs_no_improve = 0\n",
        "max_epochs = 20\n",
        "\n",
        "acc = evaluate_execution_accuracy(decoder, encoder_model, rat_encoder, dev_dataset, db_dir)\n",
        "em_score = evaluate_exact_match(decoder, encoder_model, rat_encoder, dev_dataset, db_dir)\n",
        "print(f\"Initial Dev Acc: {acc:.2%}, Exact Match: {em_score:.2%}\")\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    encoder_model.train(); rat_encoder.train(); decoder.train()\n",
        "    print(f\"\\nEpoch {epoch+1}/{max_epochs}\")\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(device_type=\"cuda\"):\n",
        "            batch['x_embed'] = batch['x_embed'].float()\n",
        "            x_all = rat_encoder(batch['x_embed'], batch['rel_mat'])\n",
        "            loss_sum = 0.0\n",
        "\n",
        "            for i in range(x_all.size(0)):\n",
        "                try:\n",
        "                    graph = batch['graphs'][i]\n",
        "                    db_id = batch['db_ids'][i]\n",
        "                    schema = schema_dict[db_id]\n",
        "                    graph['value_vocab_inv'] = value_vocab_inv\n",
        "\n",
        "                    labels = extract_labels_from_sql(batch['queries'][i], schema, value_vocab)\n",
        "                    x = x_all[i:i+1]\n",
        "\n",
        "                    loss = decoder.forward_supervised(x, graph, labels)\n",
        "                    if loss is not None and torch.isfinite(loss):\n",
        "                        loss_sum += loss\n",
        "                except Exception as e:\n",
        "                    print(f\"[Skip] Step {step}, Example {i}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if loss_sum == 0.0:\n",
        "            continue\n",
        "\n",
        "        scaler.scale(loss_sum).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), 5.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"ðŸ”¹ Step {step} - Loss: {loss_sum.item():.4f}\")\n",
        "\n",
        "\n",
        "    acc = evaluate_execution_accuracy(decoder, encoder_model, rat_encoder, dev_dataset, db_dir)\n",
        "    em_score = evaluate_exact_match(decoder, encoder_model, rat_encoder, dev_dataset, db_dir)\n",
        "    print(f\"ðŸ“Š Epoch {epoch} â€” Dev Acc: {acc:.2%}, Exact Match: {em_score:.2%}\")\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        epochs_no_improve = 0\n",
        "        torch.save({\n",
        "            'encoder': encoder_model.state_dict(),\n",
        "            'rat': rat_encoder.state_dict(),\n",
        "            'decoder': decoder.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }, \"/content/drive/MyDrive/rat_sql_best.pt\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping in\", patience, \"epochs.\")\n",
        "            break"
      ],
      "metadata": {
        "id": "JjDt4-mv1W76",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI\n",
        "import gradio as gr\n",
        "import sqlite3\n",
        "\n",
        "def extract_schema_from_sqlite(sqlite_path):\n",
        "    conn = sqlite3.connect(sqlite_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    tables = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "    schema = {\n",
        "        'table_names_original': tables,\n",
        "        'column_names_original': [],\n",
        "        'db_id': 'custom'\n",
        "    }\n",
        "\n",
        "    for i, table in enumerate(tables):\n",
        "        cursor.execute(f\"PRAGMA table_info({table});\")\n",
        "        for col in cursor.fetchall():\n",
        "            col_name = col[1]\n",
        "            schema['column_names_original'].append((i, col_name))\n",
        "\n",
        "    conn.close()\n",
        "    return schema\n",
        "\n",
        "def predict_sql_from_uploaded_db(question, db_file):\n",
        "    schema = extract_schema_from_sqlite(db_file.name)\n",
        "    graph = get_relations(question, schema, db_dir=os.path.dirname(db_file.name))\n",
        "    graph['value_vocab_inv'] = value_vocab_inv\n",
        "    tokens = graph['tokens']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_embed, _ = encoder_model([tokens])\n",
        "        rel_mat = build_relation_matrix(graph).unsqueeze(0).to(device)\n",
        "        x = rat_encoder(x_embed, rel_mat)\n",
        "        pred_sql = decoder(x, graph)\n",
        "\n",
        "    return pred_sql\n",
        "\n",
        "gr.Interface(\n",
        "    fn=predict_sql_from_uploaded_db,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Enter your natural language question\"),\n",
        "        gr.File(label=\"Upload your SQLite (.sqlite) database\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated SQL Query\"),\n",
        "    title=\"RAT-SQL Decoder (2025)\",\n",
        "    description=\"Upload a SQLite DB and ask a question. It will generate a SQL query using the RAT-SQL decoder.\"\n",
        ").launch()"
      ],
      "metadata": {
        "id": "eAr2fsl_2wDR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "9b1f2824-c247-4fae-b8b4-31ab151bfa43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://dcdbced473b64a6c08.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dcdbced473b64a6c08.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YdWRPZbuAlWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}